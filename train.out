TwoLinearModel(
  (resnet): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1), ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (6): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (7): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (8): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (9): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (10): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (11): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (12): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (13): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (14): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (15): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (16): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (17): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (18): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (19): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (20): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (21): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (22): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0, ceil_mode=False, count_include_pad=True)
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (fc1): Linear(in_features=2048, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=61, bias=True)
)
TRAIN in epoch 0 , step 0, loss = 4.11429071426
TRAIN in epoch 0 , step 50, loss = 3.67159843445
TRAIN in epoch 0 , step 100, loss = 2.99347138405
TRAIN in epoch 0 , step 150, loss = 3.00708818436
TRAIN in epoch 0 , step 200, loss = 2.77608990669
TRAIN in epoch 0 , step 250, loss = 2.40238428116
TRAIN in epoch 0 , step 300, loss = 1.9573700428
TRAIN in epoch 0 , step 350, loss = 2.51957893372
TRAIN in epoch 0 , step 400, loss = 2.58913850784
TRAIN in epoch 0 , step 450, loss = 2.73096323013
TRAIN in epoch 0 , step 500, loss = 2.02261757851
TRAIN in epoch 0 , step 550, loss = 2.81863427162
TRAIN in epoch 0 , step 600, loss = 2.3400375843
TRAIN in epoch 0 , step 650, loss = 2.94061303139
TRAIN in epoch 0 , step 700, loss = 2.18026924133
TRAIN in epoch 0 , step 750, loss = 2.68993926048
TEST in epoch0, loss = 2.20424089796, acc = 44.1348808978%
TRAIN in epoch 1 , step 0, loss = 2.44951796532
TRAIN in epoch 1 , step 50, loss = 1.70413887501
TRAIN in epoch 1 , step 100, loss = 2.28562426567
TRAIN in epoch 1 , step 150, loss = 2.12818098068
TRAIN in epoch 1 , step 200, loss = 2.52336716652
TRAIN in epoch 1 , step 250, loss = 2.01169466972
TRAIN in epoch 1 , step 300, loss = 2.07500004768
TRAIN in epoch 1 , step 350, loss = 1.67755961418
TRAIN in epoch 1 , step 400, loss = 2.17282533646
TRAIN in epoch 1 , step 450, loss = 2.19947767258
TRAIN in epoch 1 , step 500, loss = 1.7919075489
TRAIN in epoch 1 , step 550, loss = 1.99975919724
TRAIN in epoch 1 , step 600, loss = 2.08886885643
TRAIN in epoch 1 , step 650, loss = 1.68843829632
TRAIN in epoch 1 , step 700, loss = 2.21892619133
TRAIN in epoch 1 , step 750, loss = 1.72393465042
TEST in epoch1, loss = 2.09527616118, acc = 46.5277546715%
TRAIN in epoch 2 , step 0, loss = 1.84814691544
TRAIN in epoch 2 , step 50, loss = 1.65847349167
TRAIN in epoch 2 , step 100, loss = 2.11411380768
TRAIN in epoch 2 , step 150, loss = 1.75667893887
TRAIN in epoch 2 , step 200, loss = 2.56507945061
TRAIN in epoch 2 , step 250, loss = 1.96368002892
TRAIN in epoch 2 , step 300, loss = 2.32431793213
TRAIN in epoch 2 , step 350, loss = 2.07664227486
TRAIN in epoch 2 , step 400, loss = 2.19286155701
TRAIN in epoch 2 , step 450, loss = 2.13283824921
TRAIN in epoch 2 , step 500, loss = 2.12798762321
TRAIN in epoch 2 , step 550, loss = 2.11623454094
TRAIN in epoch 2 , step 600, loss = 2.05240988731
TRAIN in epoch 2 , step 650, loss = 2.18148732185
TRAIN in epoch 2 , step 700, loss = 1.61330413818
TRAIN in epoch 2 , step 750, loss = 1.58852314949
TEST in epoch2, loss = 2.05453707973, acc = 47.9830660718%
TRAIN in epoch 3 , step 0, loss = 2.12889909744
TRAIN in epoch 3 , step 50, loss = 1.80396842957
TRAIN in epoch 3 , step 100, loss = 2.22765421867
TRAIN in epoch 3 , step 150, loss = 1.86502897739
TRAIN in epoch 3 , step 200, loss = 1.81187283993
TRAIN in epoch 3 , step 250, loss = 1.78740060329
TRAIN in epoch 3 , step 300, loss = 1.70643687248
TRAIN in epoch 3 , step 350, loss = 1.64481294155
TRAIN in epoch 3 , step 400, loss = 2.29542160034
TRAIN in epoch 3 , step 450, loss = 2.25638914108
TRAIN in epoch 3 , step 500, loss = 1.5066806078
TRAIN in epoch 3 , step 550, loss = 1.8019298315
TRAIN in epoch 3 , step 600, loss = 2.12207341194
TRAIN in epoch 3 , step 650, loss = 1.70206534863
TRAIN in epoch 3 , step 700, loss = 1.83104932308
TRAIN in epoch 3 , step 750, loss = 1.48482179642
TEST in epoch3, loss = 2.00783739929, acc = 49.3500066533%
TRAIN in epoch 4 , step 0, loss = 1.24742376804
TRAIN in epoch 4 , step 50, loss = 1.77038931847
TRAIN in epoch 4 , step 100, loss = 1.46534276009
TRAIN in epoch 4 , step 150, loss = 1.73184990883
TRAIN in epoch 4 , step 200, loss = 1.52067768574
TRAIN in epoch 4 , step 250, loss = 1.45191693306
TRAIN in epoch 4 , step 300, loss = 1.66801452637
TRAIN in epoch 4 , step 350, loss = 1.75871479511
TRAIN in epoch 4 , step 400, loss = 1.85417985916
TRAIN in epoch 4 , step 450, loss = 1.80138504505
TRAIN in epoch 4 , step 500, loss = 1.95459198952
TRAIN in epoch 4 , step 550, loss = 2.19591712952
TRAIN in epoch 4 , step 600, loss = 1.49933743477
TRAIN in epoch 4 , step 650, loss = 1.24689495564
TRAIN in epoch 4 , step 700, loss = 1.54414975643
TRAIN in epoch 4 , step 750, loss = 1.61383104324
TEST in epoch4, loss = 2.03266734133, acc = 48.8493080043%
TRAIN in epoch 5 , step 0, loss = 1.55999433994
TRAIN in epoch 5 , step 50, loss = 1.39401865005
TRAIN in epoch 5 , step 100, loss = 1.2996237278
TRAIN in epoch 5 , step 150, loss = 1.63740468025
TRAIN in epoch 5 , step 200, loss = 1.45967590809
TRAIN in epoch 5 , step 250, loss = 1.56212592125
TRAIN in epoch 5 , step 300, loss = 2.01556229591
TRAIN in epoch 5 , step 350, loss = 1.25950229168
TRAIN in epoch 5 , step 400, loss = 1.57279765606
TRAIN in epoch 5 , step 450, loss = 1.63481926918
TRAIN in epoch 5 , step 500, loss = 1.3638061285
TRAIN in epoch 5 , step 550, loss = 1.37788975239
TRAIN in epoch 5 , step 600, loss = 1.82432961464
TRAIN in epoch 5 , step 650, loss = 1.02619612217
TRAIN in epoch 5 , step 700, loss = 1.42266404629
TRAIN in epoch 5 , step 750, loss = 1.39532876015
TEST in epoch5, loss = 2.04045294253, acc = 49.5833056097%
TRAIN in epoch 6 , step 0, loss = 1.25006389618
TRAIN in epoch 6 , step 50, loss = 1.21288454533
TRAIN in epoch 6 , step 100, loss = 1.94518864155
TRAIN in epoch 6 , step 150, loss = 1.71854221821
TRAIN in epoch 6 , step 200, loss = 1.3000690937
TRAIN in epoch 6 , step 250, loss = 1.67717778683
TRAIN in epoch 6 , step 300, loss = 1.30711579323
TRAIN in epoch 6 , step 350, loss = 1.75560605526
TRAIN in epoch 6 , step 400, loss = 1.58462738991
TRAIN in epoch 6 , step 450, loss = 1.31634819508
TRAIN in epoch 6 , step 500, loss = 1.30219364166
TRAIN in epoch 6 , step 550, loss = 1.97165048122
TRAIN in epoch 6 , step 600, loss = 1.42109537125
TRAIN in epoch 6 , step 650, loss = 1.00964260101
TRAIN in epoch 6 , step 700, loss = 1.44518899918
TRAIN in epoch 6 , step 750, loss = 1.55501067638
TEST in epoch6, loss = 2.08356257734, acc = 49.2576851393%
TRAIN in epoch 7 , step 0, loss = 1.5682516098
TRAIN in epoch 7 , step 50, loss = 1.60403728485
TRAIN in epoch 7 , step 100, loss = 0.952265560627
TRAIN in epoch 7 , step 150, loss = 1.47061681747
TRAIN in epoch 7 , step 200, loss = 0.969991087914
TRAIN in epoch 7 , step 250, loss = 1.27169847488
TRAIN in epoch 7 , step 300, loss = 1.71821892262
TRAIN in epoch 7 , step 350, loss = 1.35118603706
TRAIN in epoch 7 , step 400, loss = 1.4356470108
TRAIN in epoch 7 , step 450, loss = 1.3116350174
TRAIN in epoch 7 , step 500, loss = 1.27319335938
TRAIN in epoch 7 , step 550, loss = 1.72968411446
TRAIN in epoch 7 , step 600, loss = 1.76209509373
TRAIN in epoch 7 , step 650, loss = 1.45860636234
TRAIN in epoch 7 , step 700, loss = 1.34338152409
TRAIN in epoch 7 , step 750, loss = 0.990580916405
TEST in epoch7, loss = 2.10387562922, acc = 49.450229558%
TRAIN in epoch 8 , step 0, loss = 1.19670248032
TRAIN in epoch 8 , step 50, loss = 1.09435045719
TRAIN in epoch 8 , step 100, loss = 1.23715782166
TRAIN in epoch 8 , step 150, loss = 1.07401418686
TRAIN in epoch 8 , step 200, loss = 1.1247959137
TRAIN in epoch 8 , step 250, loss = 1.08992958069
TRAIN in epoch 8 , step 300, loss = 1.28951537609
TRAIN in epoch 8 , step 350, loss = 1.19982802868
TRAIN in epoch 8 , step 400, loss = 1.74377942085
TRAIN in epoch 8 , step 450, loss = 1.54950916767
TRAIN in epoch 8 , step 500, loss = 1.10815954208
TRAIN in epoch 8 , step 550, loss = 1.31461644173
TRAIN in epoch 8 , step 600, loss = 0.984674930573
TRAIN in epoch 8 , step 650, loss = 0.995460927486
TRAIN in epoch 8 , step 700, loss = 1.06151747704
TRAIN in epoch 8 , step 750, loss = 1.03773605824
TEST in epoch8, loss = 2.13468592628, acc = 49.1763839915%
TRAIN in epoch 9 , step 0, loss = 1.49056529999
TRAIN in epoch 9 , step 50, loss = 1.06153893471
TRAIN in epoch 9 , step 100, loss = 1.23287534714
TRAIN in epoch 9 , step 150, loss = 1.10415077209
TRAIN in epoch 9 , step 200, loss = 0.911523580551
TRAIN in epoch 9 , step 250, loss = 1.28656589985
TRAIN in epoch 9 , step 300, loss = 1.29513812065
TRAIN in epoch 9 , step 350, loss = 0.875235557556
TRAIN in epoch 9 , step 400, loss = 1.24565958977
TRAIN in epoch 9 , step 450, loss = 1.31462562084
TRAIN in epoch 9 , step 500, loss = 1.23225593567
TRAIN in epoch 9 , step 550, loss = 0.977562963963
TRAIN in epoch 9 , step 600, loss = 1.41184926033
TRAIN in epoch 9 , step 650, loss = 1.22560429573
TRAIN in epoch 9 , step 700, loss = 1.36817121506
TRAIN in epoch 9 , step 750, loss = 0.941478073597
TEST in epoch9, loss = 2.17885227707, acc = 48.9181332787%
TRAIN in epoch 10 , step 0, loss = 1.14222836494
TRAIN in epoch 10 , step 50, loss = 0.733144044876
TRAIN in epoch 10 , step 100, loss = 0.933056414127
TRAIN in epoch 10 , step 150, loss = 0.955183267593
TRAIN in epoch 10 , step 200, loss = 1.05367016792
TRAIN in epoch 10 , step 250, loss = 1.62732851505
TRAIN in epoch 10 , step 300, loss = 0.99589163065
TRAIN in epoch 10 , step 350, loss = 1.06822180748
TRAIN in epoch 10 , step 400, loss = 1.63772547245
TRAIN in epoch 10 , step 450, loss = 0.9430757761
TRAIN in epoch 10 , step 500, loss = 1.67717134953
TRAIN in epoch 10 , step 550, loss = 1.13402915001
TRAIN in epoch 10 , step 600, loss = 1.35089230537
TRAIN in epoch 10 , step 650, loss = 0.926694512367
TRAIN in epoch 10 , step 700, loss = 2.26487350464
TRAIN in epoch 10 , step 750, loss = 1.20665514469
TEST in epoch10, loss = 2.21897351327, acc = 48.9630464441%
TRAIN in epoch 11 , step 0, loss = 0.709384560585
TRAIN in epoch 11 , step 50, loss = 0.970418453217
TRAIN in epoch 11 , step 100, loss = 0.867759108543
TRAIN in epoch 11 , step 150, loss = 1.51701426506
TRAIN in epoch 11 , step 200, loss = 0.942063868046
TRAIN in epoch 11 , step 250, loss = 1.60881853104
TRAIN in epoch 11 , step 300, loss = 0.768840193748
TRAIN in epoch 11 , step 350, loss = 0.756738781929
TRAIN in epoch 11 , step 400, loss = 0.946006119251
TRAIN in epoch 11 , step 450, loss = 1.30487084389
TRAIN in epoch 11 , step 500, loss = 1.13650345802
TRAIN in epoch 11 , step 550, loss = 1.0013654232
TRAIN in epoch 11 , step 600, loss = 1.29493665695
TRAIN in epoch 11 , step 650, loss = 0.906241178513
TRAIN in epoch 11 , step 700, loss = 0.964109659195
TRAIN in epoch 11 , step 750, loss = 0.845179200172
TEST in epoch11, loss = 2.27098822533, acc = 48.6507335812%
TRAIN in epoch 12 , step 0, loss = 0.620741963387
TRAIN in epoch 12 , step 50, loss = 1.31733512878
TRAIN in epoch 12 , step 100, loss = 0.781744241714
TRAIN in epoch 12 , step 150, loss = 0.854970216751
TRAIN in epoch 12 , step 200, loss = 0.64390707016
TRAIN in epoch 12 , step 250, loss = 1.07069003582
TRAIN in epoch 12 , step 300, loss = 1.21737897396
TRAIN in epoch 12 , step 350, loss = 0.971273899078
TRAIN in epoch 12 , step 400, loss = 1.22907006741
TRAIN in epoch 12 , step 450, loss = 0.89210575819
TRAIN in epoch 12 , step 500, loss = 0.982233703136
TRAIN in epoch 12 , step 550, loss = 1.03930139542
TRAIN in epoch 12 , step 600, loss = 0.855808854103
TRAIN in epoch 12 , step 650, loss = 1.28904438019
TRAIN in epoch 12 , step 700, loss = 1.27233803272
TRAIN in epoch 12 , step 750, loss = 0.712159514427
TEST in epoch12, loss = 2.33864693515, acc = 48.2872696116%
TRAIN in epoch 13 , step 0, loss = 0.844516515732
TRAIN in epoch 13 , step 50, loss = 0.706360578537
TRAIN in epoch 13 , step 100, loss = 1.15434098244
TRAIN in epoch 13 , step 150, loss = 0.686096072197
TRAIN in epoch 13 , step 200, loss = 0.875085830688
TRAIN in epoch 13 , step 250, loss = 0.654410243034
TRAIN in epoch 13 , step 300, loss = 0.710305511951
TRAIN in epoch 13 , step 350, loss = 1.15187263489
TRAIN in epoch 13 , step 400, loss = 0.888879776001
TRAIN in epoch 13 , step 450, loss = 0.881284832954
TRAIN in epoch 13 , step 500, loss = 0.494294971228
TRAIN in epoch 13 , step 550, loss = 0.838016033173
TRAIN in epoch 13 , step 600, loss = 1.12119972706
TRAIN in epoch 13 , step 650, loss = 1.04090082645
TRAIN in epoch 13 , step 700, loss = 0.579594194889
TRAIN in epoch 13 , step 750, loss = 1.07892620564
TEST in epoch13, loss = 2.43660989016, acc = 47.647672834%
TRAIN in epoch 14 , step 0, loss = 1.28973126411
TRAIN in epoch 14 , step 50, loss = 0.960737109184
TRAIN in epoch 14 , step 100, loss = 0.813658714294
TRAIN in epoch 14 , step 150, loss = 0.628993988037
TRAIN in epoch 14 , step 200, loss = 0.846654415131
TRAIN in epoch 14 , step 250, loss = 0.804780900478
TRAIN in epoch 14 , step 300, loss = 0.834233701229
TRAIN in epoch 14 , step 350, loss = 0.8258446455
TRAIN in epoch 14 , step 400, loss = 1.12314021587
TRAIN in epoch 14 , step 450, loss = 0.85457777977
TRAIN in epoch 14 , step 500, loss = 0.466873258352
TRAIN in epoch 14 , step 550, loss = 0.74997150898
TRAIN in epoch 14 , step 600, loss = 0.973709344864
TRAIN in epoch 14 , step 650, loss = 0.800340056419
TRAIN in epoch 14 , step 700, loss = 1.03145444393
TRAIN in epoch 14 , step 750, loss = 0.750026881695
TEST in epoch14, loss = 2.49421012929, acc = 48.0202857788%
TRAIN in epoch 15 , step 0, loss = 0.824523746967
TRAIN in epoch 15 , step 50, loss = 0.740246772766
TRAIN in epoch 15 , step 100, loss = 0.629075288773
TRAIN in epoch 15 , step 150, loss = 1.21976256371
TRAIN in epoch 15 , step 200, loss = 0.324539422989
TRAIN in epoch 15 , step 250, loss = 0.780195355415
TRAIN in epoch 15 , step 300, loss = 0.421764403582
TRAIN in epoch 15 , step 350, loss = 0.649806499481
TRAIN in epoch 15 , step 400, loss = 1.0056078434
TRAIN in epoch 15 , step 450, loss = 0.627292752266
TRAIN in epoch 15 , step 500, loss = 0.96618360281
TRAIN in epoch 15 , step 550, loss = 0.977252840996
TRAIN in epoch 15 , step 600, loss = 0.72580742836
TRAIN in epoch 15 , step 650, loss = 0.829847335815
TRAIN in epoch 15 , step 700, loss = 0.733140647411
TRAIN in epoch 15 , step 750, loss = 0.648921787739
TEST in epoch15, loss = 2.54783426314, acc = 48.2818633946%
TRAIN in epoch 16 , step 0, loss = 0.930814325809
TRAIN in epoch 16 , step 50, loss = 0.513486623764
TRAIN in epoch 16 , step 100, loss = 0.45311191678
TRAIN in epoch 16 , step 150, loss = 0.769369184971
TRAIN in epoch 16 , step 200, loss = 0.750002145767
TRAIN in epoch 16 , step 250, loss = 0.267282903194
TRAIN in epoch 16 , step 300, loss = 0.692109286785
TRAIN in epoch 16 , step 350, loss = 0.296052753925
TRAIN in epoch 16 , step 400, loss = 0.761681079865
TRAIN in epoch 16 , step 450, loss = 0.711183607578
TRAIN in epoch 16 , step 500, loss = 0.611401081085
TRAIN in epoch 16 , step 550, loss = 1.20802748203
TRAIN in epoch 16 , step 600, loss = 0.655916333199
TRAIN in epoch 16 , step 650, loss = 0.660035133362
TRAIN in epoch 16 , step 700, loss = 0.230814114213
TRAIN in epoch 16 , step 750, loss = 0.558060050011
TEST in epoch16, loss = 2.71121434918, acc = 47.0850106491%
TRAIN in epoch 17 , step 0, loss = 0.424960017204
TRAIN in epoch 17 , step 50, loss = 0.696072995663
TRAIN in epoch 17 , step 100, loss = 0.736720323563
TRAIN in epoch 17 , step 150, loss = 0.548872113228
TRAIN in epoch 17 , step 200, loss = 1.78694713116
TRAIN in epoch 17 , step 250, loss = 0.744727194309
TRAIN in epoch 17 , step 300, loss = 0.684435844421
TRAIN in epoch 17 , step 350, loss = 0.445387244225
TRAIN in epoch 17 , step 400, loss = 1.23584318161
TRAIN in epoch 17 , step 450, loss = 0.395919501781
TRAIN in epoch 17 , step 500, loss = 0.534812450409
TRAIN in epoch 17 , step 550, loss = 0.509409070015
TRAIN in epoch 17 , step 600, loss = 0.81315433979
TRAIN in epoch 17 , step 650, loss = 0.44833111763
TRAIN in epoch 17 , step 700, loss = 0.659787833691
TRAIN in epoch 17 , step 750, loss = 0.688697695732
TEST in epoch17, loss = 2.81447440017, acc = 46.8188585409%
TRAIN in epoch 18 , step 0, loss = 0.409601390362
TRAIN in epoch 18 , step 50, loss = 0.458006501198
TRAIN in epoch 18 , step 100, loss = 0.440228372812
TRAIN in epoch 18 , step 150, loss = 0.361574202776
TRAIN in epoch 18 , step 200, loss = 0.728300750256
TRAIN in epoch 18 , step 250, loss = 0.811949133873
TRAIN in epoch 18 , step 300, loss = 0.288526803255
TRAIN in epoch 18 , step 350, loss = 0.575636684895
TRAIN in epoch 18 , step 400, loss = 0.554224848747
TRAIN in epoch 18 , step 450, loss = 0.335983335972
TRAIN in epoch 18 , step 500, loss = 0.752938508987
TRAIN in epoch 18 , step 550, loss = 0.627074480057
TRAIN in epoch 18 , step 600, loss = 0.674258112907
TRAIN in epoch 18 , step 650, loss = 0.345044255257
TRAIN in epoch 18 , step 700, loss = 0.369487583637
TRAIN in epoch 18 , step 750, loss = 0.884696424007
TEST in epoch18, loss = 2.88027163779, acc = 46.8560782431%
TRAIN in epoch 19 , step 0, loss = 0.665338873863
TRAIN in epoch 19 , step 50, loss = 0.306473642588
TRAIN in epoch 19 , step 100, loss = 0.622685551643
TRAIN in epoch 19 , step 150, loss = 0.275541812181
TRAIN in epoch 19 , step 200, loss = 0.364428877831
TRAIN in epoch 19 , step 250, loss = 0.348969966173
TRAIN in epoch 19 , step 300, loss = 0.176750019193
TRAIN in epoch 19 , step 350, loss = 0.255486756563
TRAIN in epoch 19 , step 400, loss = 0.418989390135
TRAIN in epoch 19 , step 450, loss = 0.408576369286
TRAIN in epoch 19 , step 500, loss = 0.262097030878
TRAIN in epoch 19 , step 550, loss = 0.398540794849
TRAIN in epoch 19 , step 600, loss = 0.83754992485
TRAIN in epoch 19 , step 650, loss = 0.272075384855
TRAIN in epoch 19 , step 700, loss = 0.224553093314
TRAIN in epoch 19 , step 750, loss = 0.23623482883
TEST in epoch19, loss = 2.99912570747, acc = 46.562063345%
TRAIN in epoch 20 , step 0, loss = 0.297135740519
TRAIN in epoch 20 , step 50, loss = 0.255450069904
TRAIN in epoch 20 , step 100, loss = 0.221467643976
TRAIN in epoch 20 , step 150, loss = 0.528978049755
TRAIN in epoch 20 , step 200, loss = 0.411318808794
TRAIN in epoch 20 , step 250, loss = 0.444145143032
TRAIN in epoch 20 , step 300, loss = 0.164340108633
TRAIN in epoch 20 , step 350, loss = 0.298461347818
TRAIN in epoch 20 , step 400, loss = 0.167288571596
TRAIN in epoch 20 , step 450, loss = 0.616219341755
TRAIN in epoch 20 , step 500, loss = 0.509530127048
TRAIN in epoch 20 , step 550, loss = 0.529742360115
TRAIN in epoch 20 , step 600, loss = 0.467482119799
TRAIN in epoch 20 , step 650, loss = 0.40708437562
TRAIN in epoch 20 , step 700, loss = 0.396748483181
TRAIN in epoch 20 , step 750, loss = 0.448922932148
TEST in epoch20, loss = 3.11883202154, acc = 46.6346313824%
TRAIN in epoch 21 , step 0, loss = 0.382083177567
TRAIN in epoch 21 , step 50, loss = 0.217430889606
TRAIN in epoch 21 , step 100, loss = 0.555797338486
TRAIN in epoch 21 , step 150, loss = 0.132857337594
TRAIN in epoch 21 , step 200, loss = 0.238008022308
TRAIN in epoch 21 , step 250, loss = 0.200725391507
TRAIN in epoch 21 , step 300, loss = 0.380583405495
TRAIN in epoch 21 , step 350, loss = 0.244362980127
TRAIN in epoch 21 , step 400, loss = 0.611392021179
TRAIN in epoch 21 , step 450, loss = 0.309732526541
TRAIN in epoch 21 , step 500, loss = 0.414649993181
TRAIN in epoch 21 , step 550, loss = 0.280002206564
TRAIN in epoch 21 , step 600, loss = 0.298995882273
TRAIN in epoch 21 , step 650, loss = 0.624029040337
TRAIN in epoch 21 , step 700, loss = 0.313377410173
TRAIN in epoch 21 , step 750, loss = 0.219275221229
TEST in epoch21, loss = 3.24505114043, acc = 45.7178621304%
TRAIN in epoch 22 , step 0, loss = 0.321438491344
TRAIN in epoch 22 , step 50, loss = 0.363411545753
TRAIN in epoch 22 , step 100, loss = 0.262325137854
TRAIN in epoch 22 , step 150, loss = 0.224467396736
TRAIN in epoch 22 , step 200, loss = 0.116142690182
TRAIN in epoch 22 , step 250, loss = 0.23699632287
TRAIN in epoch 22 , step 300, loss = 0.159059524536
TRAIN in epoch 22 , step 350, loss = 0.25576838851
TRAIN in epoch 22 , step 400, loss = 0.289606481791
TRAIN in epoch 22 , step 450, loss = 0.199678122997
TRAIN in epoch 22 , step 500, loss = 0.494792461395
TRAIN in epoch 22 , step 550, loss = 0.244783863425
TRAIN in epoch 22 , step 600, loss = 0.162775695324
TRAIN in epoch 22 , step 650, loss = 0.534731090069
TRAIN in epoch 22 , step 700, loss = 0.418771922588
TRAIN in epoch 22 , step 750, loss = 0.299857884645
TEST in epoch22, loss = 3.37815721659, acc = 46.6279775762%
TRAIN in epoch 23 , step 0, loss = 0.137224465609
TRAIN in epoch 23 , step 50, loss = 0.0886650681496
TRAIN in epoch 23 , step 100, loss = 0.115016624331
TRAIN in epoch 23 , step 150, loss = 0.593801856041
TRAIN in epoch 23 , step 200, loss = 0.141043096781
TRAIN in epoch 23 , step 250, loss = 0.371517688036
TRAIN in epoch 23 , step 300, loss = 0.250609517097
TRAIN in epoch 23 , step 350, loss = 0.0759989470243
TRAIN in epoch 23 , step 400, loss = 0.126060009003
TRAIN in epoch 23 , step 450, loss = 0.211832284927
TRAIN in epoch 23 , step 500, loss = 0.160197660327
TRAIN in epoch 23 , step 550, loss = 0.114917248487
TRAIN in epoch 23 , step 600, loss = 0.207954272628
TRAIN in epoch 23 , step 650, loss = 0.214738577604
TRAIN in epoch 23 , step 700, loss = 0.193362742662
TRAIN in epoch 23 , step 750, loss = 0.141702324152
TEST in epoch23, loss = 3.49707675508, acc = 45.9062479214%
TRAIN in epoch 24 , step 0, loss = 0.126950025558
TRAIN in epoch 24 , step 50, loss = 0.258291870356
TRAIN in epoch 24 , step 100, loss = 0.0688320398331
TRAIN in epoch 24 , step 150, loss = 0.150010049343
TRAIN in epoch 24 , step 200, loss = 0.178668707609
TRAIN in epoch 24 , step 250, loss = 0.29792252183
TRAIN in epoch 24 , step 300, loss = 0.203099370003
TRAIN in epoch 24 , step 350, loss = 0.170361191034
TRAIN in epoch 24 , step 400, loss = 0.126604229212
TRAIN in epoch 24 , step 450, loss = 0.11865016818
TRAIN in epoch 24 , step 500, loss = 0.34418335557
TRAIN in epoch 24 , step 550, loss = 0.138933181763
TRAIN in epoch 24 , step 600, loss = 0.113089799881
TRAIN in epoch 24 , step 650, loss = 0.108667165041
TRAIN in epoch 24 , step 700, loss = 0.076733648777
TRAIN in epoch 24 , step 750, loss = 0.162789300084
TEST in epoch24, loss = 3.63973577873, acc = 45.5309318658%
